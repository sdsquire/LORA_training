{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdsquires/.local/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-08-16 00:32:00.291809: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-16 00:32:00.408065: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-16 00:32:00.408995: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-16 00:32:01.906664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/sdsquires/.local/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdsquires/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099cd851f61a45a6b37e9057416bebea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  74%|#######4  | 199M/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "id2label = {0: 'Negative', 1: 'Positive'}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=len(id2label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d4034558cb48c4a811d6fe349f5bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c847e8083cf42ef8756f87eedf64ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049ac23fa79242e0ac6223e01cf7d3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/853k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5dab35fcf148568dfb9cec62e4296c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1f78bc98624c299765ad0ed0916ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('shawhin/imdb-truncated')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdsquires/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df8f59a9760442c1a5535705c9d18bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0122a2fd3e5b43649430478ee0d41faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fc14ebd7c24965a8287b191e8d8e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522e74f9bb5945cab862c5615a236bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aaaf22724a2457faf4cfc228d628ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    text = examples['text']\n",
    "    tokenizer.truncation_side = 'left'\n",
    "\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        return_tensors='np',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9158981bea474c60bb68baec1b7f5fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy': accuracy.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained predictions:\n",
      "--------------------\n",
      "It was terrible - Positive\n",
      "This film is great - Positive\n",
      "Not worth watching even once - Positive\n",
      "Best film ever - Positive\n",
      "Better than the first one - Positive\n",
      "Great choice if you want to waste your time - Positive\n",
      "This is a pass - Positive\n",
      "Intricate and beautiful - Positive\n"
     ]
    }
   ],
   "source": [
    "sample_text = [\n",
    "    \"It was terrible\",\n",
    "    \"This film is great\",\n",
    "    \"Not worth watching even once\",\n",
    "    \"Best film ever\",\n",
    "    \"Better than the first one\",\n",
    "    \"Great choice if you want to waste your time\",\n",
    "    \"This is a pass\",\n",
    "    \"Intricate and beautiful\",\n",
    "]\n",
    "\n",
    "print('Untrained predictions:')\n",
    "print('-'*20)\n",
    "\n",
    "for text in sample_text:\n",
    "    inputs = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.argmax(logits)\n",
    "    print(text + ' - ' + id2label[predictions.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9307\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a700c87aa5a44aa8895e82dc93af3f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7293, 'learning_rate': 0.0009866666666666667, 'epoch': 0.04}\n",
      "{'loss': 0.8629, 'learning_rate': 0.0009733333333333334, 'epoch': 0.08}\n",
      "{'loss': 0.7215, 'learning_rate': 0.00096, 'epoch': 0.12}\n",
      "{'loss': 0.6931, 'learning_rate': 0.0009466666666666667, 'epoch': 0.16}\n",
      "{'loss': 0.6209, 'learning_rate': 0.0009333333333333333, 'epoch': 0.2}\n",
      "{'loss': 0.5877, 'learning_rate': 0.00092, 'epoch': 0.24}\n",
      "{'loss': 0.5334, 'learning_rate': 0.0009066666666666666, 'epoch': 0.28}\n",
      "{'loss': 0.5183, 'learning_rate': 0.0008933333333333333, 'epoch': 0.32}\n",
      "{'loss': 0.5172, 'learning_rate': 0.00088, 'epoch': 0.36}\n",
      "{'loss': 0.3398, 'learning_rate': 0.0008666666666666667, 'epoch': 0.4}\n",
      "{'loss': 0.4424, 'learning_rate': 0.0008533333333333334, 'epoch': 0.44}\n",
      "{'loss': 0.7029, 'learning_rate': 0.00084, 'epoch': 0.48}\n",
      "{'loss': 0.351, 'learning_rate': 0.0008266666666666666, 'epoch': 0.52}\n",
      "{'loss': 0.4129, 'learning_rate': 0.0008133333333333333, 'epoch': 0.56}\n",
      "{'loss': 0.5478, 'learning_rate': 0.0008, 'epoch': 0.6}\n",
      "{'loss': 0.4004, 'learning_rate': 0.0007866666666666666, 'epoch': 0.64}\n",
      "{'loss': 0.2289, 'learning_rate': 0.0007733333333333333, 'epoch': 0.68}\n",
      "{'loss': 0.5202, 'learning_rate': 0.00076, 'epoch': 0.72}\n",
      "{'loss': 0.5191, 'learning_rate': 0.0007466666666666667, 'epoch': 0.76}\n",
      "{'loss': 0.4643, 'learning_rate': 0.0007333333333333333, 'epoch': 0.8}\n",
      "{'loss': 0.4474, 'learning_rate': 0.0007199999999999999, 'epoch': 0.84}\n",
      "{'loss': 0.2343, 'learning_rate': 0.0007066666666666666, 'epoch': 0.88}\n",
      "{'loss': 0.6383, 'learning_rate': 0.0006933333333333333, 'epoch': 0.92}\n",
      "{'loss': 0.3217, 'learning_rate': 0.00068, 'epoch': 0.96}\n",
      "{'loss': 1.1078, 'learning_rate': 0.0006666666666666666, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5412166a6351469fb4675a8f6bb73460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.868}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4001295268535614, 'eval_accuracy': {'accuracy': 0.868}, 'eval_runtime': 11.4126, 'eval_samples_per_second': 87.623, 'eval_steps_per_second': 21.906, 'epoch': 1.0}\n",
      "{'loss': 0.1628, 'learning_rate': 0.0006533333333333333, 'epoch': 1.04}\n",
      "{'loss': 0.418, 'learning_rate': 0.00064, 'epoch': 1.08}\n",
      "{'loss': 0.3456, 'learning_rate': 0.0006266666666666668, 'epoch': 1.12}\n",
      "{'loss': 0.1987, 'learning_rate': 0.0006133333333333334, 'epoch': 1.16}\n",
      "{'loss': 0.2395, 'learning_rate': 0.0006, 'epoch': 1.2}\n",
      "{'loss': 0.1368, 'learning_rate': 0.0005866666666666667, 'epoch': 1.24}\n",
      "{'loss': 0.2601, 'learning_rate': 0.0005733333333333334, 'epoch': 1.28}\n",
      "{'loss': 0.3229, 'learning_rate': 0.0005600000000000001, 'epoch': 1.32}\n",
      "{'loss': 0.2135, 'learning_rate': 0.0005466666666666667, 'epoch': 1.36}\n",
      "{'loss': 0.3367, 'learning_rate': 0.0005333333333333334, 'epoch': 1.4}\n",
      "{'loss': 0.473, 'learning_rate': 0.0005200000000000001, 'epoch': 1.44}\n",
      "{'loss': 0.4849, 'learning_rate': 0.0005066666666666668, 'epoch': 1.48}\n",
      "{'loss': 0.4062, 'learning_rate': 0.0004933333333333334, 'epoch': 1.52}\n",
      "{'loss': 0.2705, 'learning_rate': 0.00048, 'epoch': 1.56}\n",
      "{'loss': 0.4862, 'learning_rate': 0.00046666666666666666, 'epoch': 1.6}\n",
      "{'loss': 0.2793, 'learning_rate': 0.0004533333333333333, 'epoch': 1.64}\n",
      "{'loss': 0.2896, 'learning_rate': 0.00044, 'epoch': 1.68}\n",
      "{'loss': 0.2651, 'learning_rate': 0.0004266666666666667, 'epoch': 1.72}\n",
      "{'loss': 0.2544, 'learning_rate': 0.0004133333333333333, 'epoch': 1.76}\n",
      "{'loss': 0.2995, 'learning_rate': 0.0004, 'epoch': 1.8}\n",
      "{'loss': 0.2853, 'learning_rate': 0.00038666666666666667, 'epoch': 1.84}\n",
      "{'loss': 0.4627, 'learning_rate': 0.0003733333333333334, 'epoch': 1.88}\n",
      "{'loss': 0.584, 'learning_rate': 0.00035999999999999997, 'epoch': 1.92}\n",
      "{'loss': 0.1203, 'learning_rate': 0.00034666666666666667, 'epoch': 1.96}\n",
      "{'loss': 0.2439, 'learning_rate': 0.0003333333333333333, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36ab0295f764434b9a5d6835f2bc219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.889}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33785662055015564, 'eval_accuracy': {'accuracy': 0.889}, 'eval_runtime': 11.9569, 'eval_samples_per_second': 83.634, 'eval_steps_per_second': 20.908, 'epoch': 2.0}\n",
      "{'loss': 0.3313, 'learning_rate': 0.00032, 'epoch': 2.04}\n",
      "{'loss': 0.1662, 'learning_rate': 0.0003066666666666667, 'epoch': 2.08}\n",
      "{'loss': 0.1909, 'learning_rate': 0.0002933333333333333, 'epoch': 2.12}\n",
      "{'loss': 0.4892, 'learning_rate': 0.00028000000000000003, 'epoch': 2.16}\n",
      "{'loss': 0.2353, 'learning_rate': 0.0002666666666666667, 'epoch': 2.2}\n",
      "{'loss': 0.1457, 'learning_rate': 0.0002533333333333334, 'epoch': 2.24}\n",
      "{'loss': 0.4721, 'learning_rate': 0.00024, 'epoch': 2.28}\n",
      "{'loss': 0.0937, 'learning_rate': 0.00022666666666666666, 'epoch': 2.32}\n",
      "{'loss': 0.1637, 'learning_rate': 0.00021333333333333336, 'epoch': 2.36}\n",
      "{'loss': 0.2804, 'learning_rate': 0.0002, 'epoch': 2.4}\n",
      "{'loss': 0.456, 'learning_rate': 0.0001866666666666667, 'epoch': 2.44}\n",
      "{'loss': 0.4029, 'learning_rate': 0.00017333333333333334, 'epoch': 2.48}\n",
      "{'loss': 0.0584, 'learning_rate': 0.00016, 'epoch': 2.52}\n",
      "{'loss': 0.0533, 'learning_rate': 0.00014666666666666666, 'epoch': 2.56}\n",
      "{'loss': 0.3666, 'learning_rate': 0.00013333333333333334, 'epoch': 2.6}\n",
      "{'loss': 0.332, 'learning_rate': 0.00012, 'epoch': 2.64}\n",
      "{'loss': 0.5195, 'learning_rate': 0.00010666666666666668, 'epoch': 2.68}\n",
      "{'loss': 0.2274, 'learning_rate': 9.333333333333334e-05, 'epoch': 2.72}\n",
      "{'loss': 0.6097, 'learning_rate': 8e-05, 'epoch': 2.76}\n",
      "{'loss': 0.1055, 'learning_rate': 6.666666666666667e-05, 'epoch': 2.8}\n",
      "{'loss': 0.3812, 'learning_rate': 5.333333333333334e-05, 'epoch': 2.84}\n",
      "{'loss': 0.1945, 'learning_rate': 4e-05, 'epoch': 2.88}\n",
      "{'loss': 0.3094, 'learning_rate': 2.666666666666667e-05, 'epoch': 2.92}\n",
      "{'loss': 0.1037, 'learning_rate': 1.3333333333333335e-05, 'epoch': 2.96}\n",
      "{'loss': 0.2388, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb97374cd184694918cade8d3cc7824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.9}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29631802439689636, 'eval_accuracy': {'accuracy': 0.9}, 'eval_runtime': 12.1839, 'eval_samples_per_second': 82.075, 'eval_steps_per_second': 20.519, 'epoch': 3.0}\n",
      "{'train_runtime': 112.4103, 'train_samples_per_second': 26.688, 'train_steps_per_second': 6.672, 'train_loss': 0.3764014965693156, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.3764014965693156, metrics={'train_runtime': 112.4103, 'train_samples_per_second': 26.688, 'train_steps_per_second': 6.672, 'train_loss': 0.3764014965693156, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config = LoraConfig(task_type='SEQ_CLS',\n",
    "                        r=4,\n",
    "                        lora_alpha=0.5,\n",
    "                        lora_dropout=0.01,\n",
    "                        target_modules=['q_lin'],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 4\n",
    "num_epochs = 3\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_checkpoint + '-lora-text-classification',\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained predictions:\n",
      "--------------------\n",
      "It was terrible - Negative\n",
      "This film is great - Positive\n",
      "Not worth watching even once - Positive\n",
      "Best film ever - Positive\n",
      "Better than the first one - Positive\n",
      "Great choice if you want to waste your time - Positive\n",
      "This is a pass - Positive\n",
      "Intricate and beautiful - Positive\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "print('Trained predictions:')\n",
    "print('-'*20)\n",
    "for text in sample_text:\n",
    "    inputs = tokenizer.encode(text, return_tensors='pt').to('cpu')\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.argmax(logits)\n",
    "    print(text + ' - ' + id2label[predictions.tolist()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
